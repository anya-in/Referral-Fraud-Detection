# -*- coding: utf-8 -*-
"""Referral_Fraud_Detection-Data .ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1HwQm4A8a1D1TDEqK32RLeehlxYSrOYqU

#**Referral Fraud Detection – Subtask 1: Data Transformation**

I have chosen Pandas for the Data Transformation task after doing a quick comparison to see what would work best for this project. Since I’m already familiar with Pandas, and it offers straightforward syntax and handles in-memory datasets efficiently, it felt like the right starting point. Later on, I’d like to explore using PySpark to work with larger datasets and evaluate its performance.

In this subtask, I had focused on building the data transformation playground logic required for detecting referral fraud. The goal was to prepare clean, well-structured, and enriched datasets by applying essential data engineering steps such as data profiling, cleaning, timezone conversion, enrichment through joins, and validation against business rules.This prepared data will help detect referral fraud accurately.

## Step1 : Data Profiling

I had started by loading all the required CSV files and performing data profiling on each. This helped me understand the schema, detect potential issues like missing values, null patterns, and inconsistent types, which are crucial for deciding transformation and cleaning strategies in the next steps.
"""

import pandas as pd
import os

# Define the path to your data directory
data_dir = './data/'

# List all CSV files in the data directory
try:
    csv_files = [file for file in os.listdir(data_dir) if file.endswith('.csv')]
except FileNotFoundError:
    print(f"Error: The directory '{data_dir}' was not found. Make sure it exists.")
    csv_files = []


# Create a dictionary to hold DataFrames for each CSV
dataframes = {}

# Loop through each CSV file and read it into a DataFrame
for file in csv_files:
    table_name = file.replace('.csv', '')  # Use filename (without .csv) as key
    file_path = os.path.join(data_dir, file) # Construct the full path

    df = pd.read_csv(file_path)  # Read the CSV from the correct path
    dataframes[table_name] = df  # Store in dictionary

    # Print table name
    print(f"--- Profiling for: {table_name} ---")

    # Show basic shape
    print("Shape:", df.shape)

    # Show column names and data types
    print(df.dtypes)

    # Show number of null values in each column
    print("Nulls:\n", df.isnull().sum())

    # Show number of duplicate rows
    print("Duplicates:", df.duplicated().sum())

    print("\n")

"""**Output Analysis :**

 The initial data profiling process was carried out across seven datasets: lead\_log, referral\_rewards, user\_referral\_statuses, user\_logs, user\_referrals, user\_referral\_logs, and paid\_transactions. Most tables were well-structured with no missing values, duplicates, or inconsistencies in data types. The lead\_log, referral\_rewards, user\_referral\_statuses, and user\_logs tables were completely clean, showing no nulls or duplicate entries. These tables had consistent column types, primarily object and integer fields, and relatively small row counts, which indicates they may serve as reference or log tables.

Some issues were identified in the user\_referrals and user\_referral\_logs datasets. In the user\_referrals table, there were missing values in referral\_reward\_id, referrer\_id, and transaction\_id columns. Referral\_reward\_id had 38 nulls out of 46 rows, which could affect analyses involving reward mapping. Similarly, the user\_referral\_logs table had 79 null values in the source\_transaction\_id column out of 96 rows, raising concerns about the completeness of transaction trail information.

The paid\_transactions table appeared to be complete and clean, with consistent column types and no missing or duplicate data. Overall, the datasets are mostly clean and ready for further processing, but attention is needed to address the missing values in key fields that link referral and transaction data. This will help ensure accurate results during data transformation, enrichment, and fraud detection validation steps.

## Step 2 : Data Cleaning

In this step, I focused on cleaning the data based on what I observed during the profiling stage. While most of the tables looked clean and consistent, there were two tables that needed attention: **user\_referrals** and **user\_referral\_logs**. In `user_referrals`, I originally considered dropping rows where critical fields like `referrer_id` and `transaction_id` were missing, since these fields are essential for identifying who referred whom and linking the referral to a transaction. However, I ultimately commented that step out, possibly to preserve more data for further validation. For the referral_reward_id, which was often missing but not always necessary for downstream analysis, I chose to fill in a placeholder value like `'unknown'` so I wouldn’t lose potentially useful rows.

Next, for user_referral_logs, the source_transaction_id column had many missing values. Since this field might be optional and not every log entry corresponds to a transaction, I left those as-is but ensured the datetime fields like created_at were properly converted into datetime format for consistency. Finally, I made sure to remove any duplicate rows in both tables just to be safe. This cleaning process ensures that the data is ready for consistent transformation and joining in the next steps.
"""

# Step 2: Data Cleaning

# We'll clean only the affected tables: user_referrals and user_referral_logs

# Clean `user_referrals`
user_referrals = dataframes['user_referrals'].copy()

# Drop rows where essential IDs are missing (critical for joins)
# user_referrals = user_referrals.dropna(subset=['referrer_id', 'transaction_id'])

# Optional: Fill missing reward IDs with a placeholder (if reward info is not always present)
user_referrals['referral_reward_id'] = user_referrals['referral_reward_id'].fillna('unknown')

# Convert datetime fields to standard datetime format
if 'created_at' in user_referrals.columns:
    user_referrals['created_at'] = pd.to_datetime(user_referrals['created_at'])

# Clean `user_referral_logs`
user_referral_logs = dataframes['user_referral_logs'].copy()

# Keep nulls in source_transaction_id as they might represent non-transactional events
# But ensure datetime field is parsed correctly
if 'created_at' in user_referral_logs.columns:
    user_referral_logs['created_at'] = pd.to_datetime(user_referral_logs['created_at'])

# Optional: Drop duplicates if any slipped through
user_referrals = user_referrals.drop_duplicates()
user_referral_logs = user_referral_logs.drop_duplicates()

# Save cleaned versions back to the main dictionary
dataframes['user_referrals'] = user_referrals
dataframes['user_referral_logs'] = user_referral_logs

# Print confirmation
print("Data cleaning completed for user_referrals and user_referral_logs.")

"""## Step 3 : Data Enrichment (Join Tables)

In this step, I consolidated referral-related data by joining all relevant tables into a single dataframe. Since key details like referral statuses, rewards, user info, transactions, and lead activity were spread across multiple tables, I first aggregated user\_logs, user\_referral\_logs, and lead\_log to retain only the latest records. Then I standardized join keys by aligning data types and performed step-by-step merges using the cleaned and aggregated tables. I also created a referral\_source\_category column to group referral sources. This enrichment prepared the data for timezone conversion and business logic validation in the next steps.
"""

import pandas as pd
import numpy as np

# --- 1. Aggregate ALL Tables with Duplicate Join Keys ---

# Aggregate user_logs to keep only one record per user
user_logs_agg = dataframes['user_logs'].copy()
# Simple de-duplication is effective here since we just need one profile per user
user_logs_agg = user_logs_agg.drop_duplicates(subset='user_id', keep='last')

# Aggregate user_referral_logs to get only the latest entry per referral
logs_agg = dataframes['user_referral_logs'].copy()
logs_agg['created_at'] = pd.to_datetime(logs_agg['created_at'])
logs_agg = logs_agg.loc[logs_agg.groupby('user_referral_id')['created_at'].idxmax()]

# Aggregate lead_log to get only the latest entry per lead
lead_agg = dataframes['lead_log'].copy()
lead_agg['created_at'] = pd.to_datetime(lead_agg['created_at'])
lead_agg = lead_agg.loc[lead_agg.groupby('lead_id')['created_at'].idxmax()]


# --- 2. Prepare Join Keys ---

# Start with the central table
merged_df = dataframes['user_referrals'].copy()

# Convert join keys in the main DataFrame
# ... (your original astype conversions for merged_df keys) ...
merged_df['user_referral_status_id'] = merged_df['user_referral_status_id'].astype(str)
merged_df['referrer_id'] = merged_df['referrer_id'].astype(str)
merged_df['referral_reward_id'] = pd.to_numeric(
    merged_df['referral_reward_id'], errors='coerce').astype('Int64').astype(str)
merged_df['transaction_id'] = merged_df['transaction_id'].astype(str)
merged_df['referral_id'] = merged_df['referral_id'].astype(str)
merged_df['referee_id'] = merged_df['referee_id'].astype(str)


# Convert join keys in the other DataFrames
dataframes['user_referral_statuses']['id'] = dataframes['user_referral_statuses']['id'].astype(str)
dataframes['referral_rewards']['id'] = dataframes['referral_rewards']['id'].astype(str)
dataframes['paid_transactions']['transaction_id'] = dataframes['paid_transactions']['transaction_id'].astype(str)
# Use the aggregated tables
user_logs_agg['user_id'] = user_logs_agg['user_id'].astype(str)
logs_agg['user_referral_id'] = logs_agg['user_referral_id'].astype(str)
lead_agg['lead_id'] = lead_agg['lead_id'].astype(str)


# --- 3. Merge Tables Step-by-Step with Aggregated Data ---

merged_df = pd.merge(merged_df, dataframes['user_referral_statuses'], left_on='user_referral_status_id', right_on='id', how='left', suffixes=('', '_status'))
# Use the AGGREGATED user_logs_agg table
merged_df = pd.merge(merged_df, user_logs_agg, left_on='referrer_id', right_on='user_id', how='left', suffixes=('', '_referrer'))
merged_df = pd.merge(merged_df, dataframes['referral_rewards'], left_on='referral_reward_id', right_on='id', how='left', suffixes=('', '_reward'))
merged_df = pd.merge(merged_df, dataframes['paid_transactions'], on='transaction_id', how='left')
# Use the other AGGREGATED tables
merged_df = pd.merge(merged_df, logs_agg, left_on='referral_id', right_on='user_referral_id', how='left', suffixes=('', '_log'))
merged_df = pd.merge(merged_df, lead_agg, left_on='referee_id', right_on='lead_id', how='left', suffixes=('', '_lead'))


# --- 4. Create Derived Category Column ---

conditions = [
    merged_df['referral_source'] == 'User Sign Up',
    merged_df['referral_source'] == 'Draft Transaction',
    merged_df['referral_source'] == 'Lead'
]
choices = [
    'Online',
    'Offline',
    merged_df['source_category']
]
merged_df['referral_source_category'] = np.select(conditions, choices, default=None)


# --- 5. Verify the Final Shape ---

print("Final data enrichment complete.")
print(f"Shape of the final merged DataFrame: {merged_df.shape}")

"""## Step 4 : Timezone Conversion

In this step, I standardized key datetime fields by converting them to UTC. For transaction\_at, I used both the timestamp and its associated timezone to create a reliable UTC version, handling both naive and aware datetimes. For referral\_at and membership\_expired\_date, I converted them directly to UTC, adding new columns with the \_utc suffix. This ensures consistency for any time-based analysis going forward.
"""

import pandas as pd
import pytz

# --- Create Final Timezone-Aware UTC Columns ---

# Define a robust function to handle transaction_at conversion
def convert_to_utc(row):
    if pd.isna(row['transaction_at']) or pd.isna(row['timezone_transaction']):
        return pd.NaT
    try:
        # For naive datetimes
        return pd.to_datetime(row['transaction_at']).tz_localize(row['timezone_transaction']).tz_convert('UTC')
    except TypeError:
        # For datetimes that are already aware
        return pd.to_datetime(row['transaction_at']).tz_convert('UTC')
    except Exception:
        return pd.NaT

merged_df['transaction_at_utc'] = merged_df.apply(convert_to_utc, axis=1)

# Handle the other datetime columns
for col in ['referral_at', 'membership_expired_date']:
    merged_df[col] = pd.to_datetime(merged_df[col], errors='coerce')
    try:
        # For columns that are already timezone-aware
        merged_df[f'{col}_utc'] = merged_df[col].dt.tz_convert('UTC')
    except TypeError:
        # For columns that are timezone-naive
        merged_df[f'{col}_utc'] = merged_df[col].dt.tz_localize('UTC')

print("Timezone conversion complete.")
print("Created columns: 'referral_at_utc', 'transaction_at_utc', 'membership_expired_date_utc'")

"""## Step 5 : Business Logic Validation

In this step, I applied detailed business validation rules to the enriched referral data. A referral was marked valid either if it met all success criteria, such as having a valid transaction, reward, and active membership; or if it was in a pending or failed state with no reward. I also added a column to capture the specific reason for invalid cases. Out of 46 referrals processed, 38 were valid. Among the 8 invalid cases, 5 had a success status but no reward assigned, and 3 had completed transactions but were missing corresponding rewards. This helped ensure the referral logic aligns with business expectations and identified gaps for potential correction.
"""

import pandas as pd
import numpy as np

# Ensure reward_value is numeric and fill missing values with 0
merged_df['reward_value'] = pd.to_numeric(merged_df['reward_value'], errors='coerce').fillna(0)

# OPTIONAL: Check if reward granted column exists
has_granted_field = 'reward_granted_to_referee' in merged_df.columns

# Function to validate referral rows and assign invalid reason if any
def validate_with_reason(row):
    referral_time = row['referral_at_utc']
    transaction_time = row['transaction_at_utc']
    membership_expiry = row['membership_expired_date_utc']
    reward_value = row['reward_value']

    # Helper flags
    has_transaction = pd.notna(row['transaction_id']) and row['transaction_id'].lower() != 'nan'
    transaction_after_referral = transaction_time >= referral_time if pd.notna(transaction_time) and pd.notna(referral_time) else False
    is_within_30_days = (transaction_time - referral_time) <= pd.Timedelta(days=30) if pd.notna(transaction_time) and pd.notna(referral_time) else False
    same_month = (
        pd.notna(transaction_time) and pd.notna(referral_time) and
        transaction_time.month == referral_time.month and
        transaction_time.year == referral_time.year
    )
    membership_valid = pd.isna(membership_expiry) or (membership_expiry > transaction_time)
    reward_granted = row['reward_granted_to_referee'] if has_granted_field else True
    is_not_deleted = not row['is_deleted']

    # --- Valid: Condition 1 ---
    if (
        row['description'] == "Success" and
        has_transaction and
        row['transaction_status'] == "PAID" and
        row['transaction_type'] == "NEW" and
        transaction_after_referral and
        same_month and
        membership_valid and
        is_not_deleted and
        reward_value > 0 and
        reward_granted
    ):
        return pd.Series([True, None])

    # --- Valid: Condition 2 (Pending / Failed) ---
    if (
        row['description'] in ["Pending", "Failed"] and
        reward_value <= 0
    ):
        return pd.Series([True, None])

    # --- Invalid Conditions (label reason) ---
    if reward_value > 0 and row['description'] != "Success":
        return pd.Series([False, "Invalid: reward > 0 but not success"])
    if reward_value > 0 and not has_transaction:
        return pd.Series([False, "Invalid: reward > 0 but missing transaction"])
    if reward_value == 0 and has_transaction and row['transaction_status'] == "PAID" and transaction_after_referral:
        return pd.Series([False, "Invalid: transaction present but no reward"])
    if row['description'] == "Success" and reward_value == 0:
        return pd.Series([False, "Invalid: success but reward is 0 or null"])
    if not transaction_after_referral and pd.notna(transaction_time) and pd.notna(referral_time):
        return pd.Series([False, "Invalid: transaction before referral"])

    return pd.Series([False, "Invalid: other"])

# Apply the function
merged_df[['is_business_logic_valid', 'invalid_reason']] = merged_df.apply(validate_with_reason, axis=1)

# Print result summary
print("Validation complete with detailed reason tracking.")
print(f"Total referrals processed: {len(merged_df)}")
print(f"Valid referrals: {merged_df['is_business_logic_valid'].sum()}")
print("Top invalid reasons:")
print(merged_df['invalid_reason'].value_counts())

import pandas as pd
import numpy as np

# --- 1. Create Final Report Columns ---
merged_df['num_reward_days'] = 30

if 'is_reward_granted' in merged_df.columns and 'created_at_log' in merged_df.columns:
    merged_df['reward_granted_at'] = pd.to_datetime(
        np.where(merged_df['is_reward_granted'] == True, merged_df['created_at_log'], pd.NaT)
    )
else:
    # If the required columns don't exist, create an empty datetime column
    merged_df['reward_granted_at'] = pd.NaT


# --- 2. Define Column Mapping and Order ---
column_mapping = {
    'id_x': 'referral_details_id',
    'referral_id': 'referral_id',
    'referral_source': 'referral_source',
    'referral_source_category': 'referral_source_category',
    'referral_at': 'referral_at',
    'referrer_id': 'referrer_id',
    'name': 'referrer_name',
    'phone_number': 'referrer_phone_number',
    'homeclub': 'referrer_homeclub',
    'referee_id': 'referee_id',
    'referee_name': 'referee_name',
    'referee_phone': 'referee_phone',
    'description': 'referral_status',
    'num_reward_days': 'num_reward_days',
    'transaction_id': 'transaction_id',
    'transaction_status': 'transaction_status',
    'transaction_at': 'transaction_at',
    'transaction_location': 'transaction_location',
    'transaction_type': 'transaction_type',
    'updated_at': 'updated_at',
    'reward_granted_at': 'reward_granted_at',
    'is_business_logic_valid': 'is_business_logic_valid'
}

# Create the report DataFrame, only renaming columns that actually exist
existing_columns_to_rename = {k: v for k, v in column_mapping.items() if k in merged_df.columns}
final_report_df = merged_df.rename(columns=existing_columns_to_rename)


final_columns_order = [
    'referral_details_id', 'referral_id', 'referral_source', 'referral_source_category',
    'referral_at', 'referrer_id', 'referrer_name', 'referrer_phone_number', 'referrer_homeclub',
    'referee_id', 'referee_name', 'referee_phone', 'referral_status', 'num_reward_days',
    'transaction_id', 'transaction_status', 'transaction_at', 'transaction_location',
    'transaction_type', 'updated_at', 'reward_granted_at', 'is_business_logic_valid'
]

# Ensure only existing columns are used to set the final order
final_report_df = final_report_df[[col for col in final_columns_order if col in final_report_df.columns]]


# --- 3. Save Final Reports to Docker Path ---
# The path must point to the '/app/output/' directory inside the container.
output_filename = '/app/output/referral_validation_report.csv'
processed_data_filename = '/app/output/referral_data_processed.csv'

final_report_df.to_csv(output_filename, index=False, date_format='%Y-%m-%d %H:%M:%S')
merged_df.to_csv(processed_data_filename, index=False)


print(f"Report successfully generated and saved as '{output_filename}'")
print("\nFirst 5 rows of the report:")
print(final_report_df.head())

# merged_df.to_csv('referral_data_processed.csv', index=False)

"""I started by loading all the referral-related CSV files and inspecting their data quality, including shapes, missing values, and duplicates. Several critical fields like transaction\_id, transaction\_at, and referral\_reward\_id had missing or inconsistent values across tables. Early merge attempts failed due to type mismatches, such as strings versus integers in join keys, so I standardized all ID columns to consistent string types.

For data enrichment, I joined the referral data with transaction, status, user, log, and reward information, ensuring clean type handling and removing duplicates where necessary. I also created a derived column to categorize referral sources into high-level groups like online and offline.

During timezone conversion, I parsed all relevant datetime fields and localized them based on their original timezones, then converted them to UTC. This ensured consistent and accurate temporal comparisons, even when some timestamps were missing.

For business logic validation, I implemented a detailed set of rules to determine whether each referral reward was valid. This included checking reward values, referral status, transaction status and type, timing constraints, membership expiry, account deletion status, and whether a reward was granted. I also added logic to track invalid referrals with clear reason codes. Out of 46 referrals, 38 were classified as valid. The most common invalid reasons were success cases with missing rewards or completed transactions without corresponding rewards.

Throughout the process, I used diagnostic summaries, type checks, and sample inspections to ensure data quality. Finally, I saved a detailed validation report as a CSV file for further analysis.

"""